{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c96352-bd23-433c-8523-71e3076c0ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import logging\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e12e4c-9614-4081-97a1-085409ddc57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8020d6d-3356-4556-bfa3-71594244ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(1)\n",
    "torch.set_num_interop_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1c0884e-01ff-40d4-95a6-4f05419871a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(torch.get_num_threads())\n",
    "print(torch.get_num_interop_threads())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc690e80-b6a5-4256-876f-4b05266bb69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 514.70 MB\n",
      "Tokenizer size: 2.41 MB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    ")\n",
    "\n",
    "# Serialize and measure\n",
    "model_bytes = pickle.dumps(model)\n",
    "tokenizer_bytes = pickle.dumps(tokenizer)\n",
    "\n",
    "print(f\"Model size: {len(model_bytes)/1024/1024:.2f} MB\")\n",
    "print(f\"Tokenizer size: {len(tokenizer_bytes)/1024/1024:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d742beee-e12e-403b-aeef-501a0f9948fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of active threads: 18\n"
     ]
    }
   ],
   "source": [
    "import psutil, os\n",
    "p = psutil.Process(os.getpid())\n",
    "print(f\"Number of active threads: {p.num_threads()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca7ce960-eff4-46fb-babb-f1cc474149d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m bertweet_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiniteautomata/bertweet-base-sentiment-analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Compute token lengths for each review text\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m token_counts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m     17\u001b[0m         bertweet_tokenizer(\n\u001b[1;32m     18\u001b[0m             review[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         )\n\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m amazon_reviews\n\u001b[1;32m     22\u001b[0m ]\n\u001b[1;32m     23\u001b[0m average_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(token_counts) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(token_counts)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage token count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_tokens\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m bertweet_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiniteautomata/bertweet-base-sentiment-analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Compute token lengths for each review text\u001b[39;00m\n\u001b[1;32m     15\u001b[0m token_counts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m---> 17\u001b[0m         \u001b[43mbertweet_tokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreview\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m amazon_reviews\n\u001b[1;32m     22\u001b[0m ]\n\u001b[1;32m     23\u001b[0m average_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(token_counts) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(token_counts)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage token count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_tokens\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2475\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2456\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2457\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2472\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2473\u001b[0m     )\n\u001b[1;32m   2474\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2478\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2493\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2494\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2548\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2538\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2540\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2541\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2546\u001b[0m )\n\u001b[0;32m-> 2548\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2551\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2566\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils.py:646\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    640\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    644\u001b[0m     )\n\u001b[0;32m--> 646\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    650\u001b[0m     first_ids,\n\u001b[1;32m    651\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    665\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    666\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils.py:615\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 615\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils.py:516\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    513\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(pattern, \u001b[38;5;28;01mlambda\u001b[39;00m m: m\u001b[38;5;241m.\u001b[39mgroups()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m m\u001b[38;5;241m.\u001b[39mgroups()[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower(), text)\n\u001b[1;32m    515\u001b[0m no_split_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique_no_split_tokens)\n\u001b[0;32m--> 516\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens_trie\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# [\"This is something\", \"<special_token_1>\", \"  else\"]\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils.py:221\u001b[0m, in \u001b[0;36mTrie.split\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    219\u001b[0m     states \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m start \u001b[38;5;129;01min\u001b[39;00m to_remove:\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m states[start]\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# If this character is a starting character within the trie\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# start keeping track of this partial match.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "amazon_reviews = []\n",
    "# Load the JSON data (assuming it's a list of review objects)\n",
    "with open(\"Baby_Products.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        amazon_reviews.append(json.loads(line))\n",
    "\n",
    "amazon_reviews_text = [review[\"text\"] for review in amazon_reviews[:1000]]\n",
    "\n",
    "# Initialize the DistilBERT tokenizer\n",
    "bertweet_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    ")\n",
    "\n",
    "# Compute token lengths for each review text\n",
    "token_counts = [\n",
    "    len(\n",
    "        bertweet_tokenizer(\n",
    "            review[\"text\"], return_tensors=\"pt\", padding=\"max_length\", truncation=True\n",
    "        )\n",
    "    )\n",
    "    for review in amazon_reviews\n",
    "]\n",
    "average_tokens = sum(token_counts) / len(token_counts)\n",
    "\n",
    "print(f\"Average token count: {average_tokens:.1f}\")\n",
    "\n",
    "bertweet_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    ")\n",
    "bertweet_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    ")\n",
    "\n",
    "fine_tuned_sentiment_results = []\n",
    "processing_times = []\n",
    "token_counts = []\n",
    "\n",
    "print(f\"Processing {len(amazon_reviews_text)} reviews\")\n",
    "for review_text in tqdm(amazon_reviews_text, desc=\"Processing reviews\"):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Tokenize the review text\n",
    "    inputs = bertweet_tokenizer(\n",
    "        review_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True\n",
    "    )\n",
    "\n",
    "    # Calculate the actual number of tokens (using attention_mask to ignore padding)\n",
    "    token_count = inputs[\"attention_mask\"].sum().item()\n",
    "    token_counts.append(token_count)\n",
    "\n",
    "    # # Perform sentiment analysis\n",
    "    # with torch.no_grad():\n",
    "    #     sentiment_batch = bertweet_model(**inputs).logits\n",
    "\n",
    "    # predicted_class_id = sentiment_batch.argmax().item()\n",
    "    # sentiment_label = bertweet_model.config.id2label[predicted_class_id]\n",
    "\n",
    "    # fine_tuned_sentiment_results.append(\n",
    "    #     {\"review_text\": review_text, \"sentiment\": sentiment_label}\n",
    "    # )\n",
    "\n",
    "    batch_time = time.time() - start_time\n",
    "    processing_times.append(batch_time)\n",
    "\n",
    "# Compute average and maximum values\n",
    "avg_time = sum(processing_times) / len(processing_times)\n",
    "max_time = max(processing_times)\n",
    "avg_tokens = sum(token_counts) / len(token_counts)\n",
    "max_tokens = max(token_counts)\n",
    "total_batches = len(processing_times)\n",
    "total_processing_time = sum(processing_times)\n",
    "total_tokens = sum(token_counts) \n",
    "throughput = total_tokens / total_processing_time\n",
    "\n",
    "print(\"Total number of tokens:\", total_tokens)\n",
    "print(\"Total number of batches:\", total_batches)\n",
    "print(\"Total time taken: {:.4f} seconds\".format(total_processing_time))\n",
    "print(\"Average time per batch: {:.4f} seconds\".format(avg_time))\n",
    "print(\"Maximum time for a batch: {:.4f} seconds\".format(max_time))\n",
    "print(\"Average batch size (in tokens): {:.2f}\".format(avg_tokens))\n",
    "print(\"Longest batch (in tokens):\", max_tokens)\n",
    "print(\"Throughput (in tokens/second):\", throughput)\n",
    "\n",
    "# bertweet_sentiment_amazon_df = pd.DataFrame(fine_tuned_sentiment_results)\n",
    "\n",
    "# # Save the results to a CSV file\n",
    "# bertweet_sentiment_amazon_df.to_csv(\n",
    "#     \"bertweet_sentiment_analysis_amazon_results.csv\", index=False\n",
    "# )\n",
    "\n",
    "# print(\"Result saved to 'bertweet_sentiment_analysis_amazon_results.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db9eb83c-e93a-4bd2-be2d-0586a6b862ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average token count: 3.0\n",
      "Processing 1000 reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing reviews: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [03:46<00:00,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches: 1000\n",
      "Total time taken: 224.6940 seconds\n",
      "Average time per batch: 0.2247 seconds\n",
      "Maximum time for a batch: 0.3317 seconds\n",
      "Average batch size (in tokens): 55.40\n",
      "Longest batch (in tokens): 128\n",
      "Throughput (in tokens/second): 246.56195015504193\n",
      "Result saved to 'bertweet_sentiment_analysis_amazon_results.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "amazon_reviews = []\n",
    "# Load the JSON data (assuming it's a list of review objects)\n",
    "with open(\"Subscription_Boxes.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        amazon_reviews.append(json.loads(line))\n",
    "\n",
    "amazon_reviews_text = [review[\"text\"] for review in amazon_reviews[:1000]]\n",
    "\n",
    "# Initialize the DistilBERT tokenizer\n",
    "bertweet_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    ")\n",
    "\n",
    "# Compute token lengths for each review text\n",
    "token_counts = [\n",
    "    len(\n",
    "        bertweet_tokenizer(\n",
    "            review[\"text\"], return_tensors=\"pt\", padding=\"max_length\", truncation=True\n",
    "        )\n",
    "    )\n",
    "    for review in amazon_reviews\n",
    "]\n",
    "average_tokens = sum(token_counts) / len(token_counts)\n",
    "\n",
    "print(f\"Average token count: {average_tokens:.1f}\")\n",
    "\n",
    "bertweet_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    ")\n",
    "bertweet_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    ")\n",
    "\n",
    "fine_tuned_sentiment_results = []\n",
    "processing_times = []\n",
    "token_counts = []\n",
    "\n",
    "print(f\"Processing {len(amazon_reviews_text)} reviews\")\n",
    "for review_text in tqdm(amazon_reviews_text, desc=\"Processing reviews\"):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Tokenize the review text\n",
    "    inputs = bertweet_tokenizer(\n",
    "        review_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True\n",
    "    )\n",
    "\n",
    "    # Calculate the actual number of tokens (using attention_mask to ignore padding)\n",
    "    token_count = inputs[\"attention_mask\"].sum().item()\n",
    "    token_counts.append(token_count)\n",
    "\n",
    "    # Perform sentiment analysis\n",
    "    with torch.no_grad():\n",
    "        sentiment_batch = bertweet_model(**inputs).logits\n",
    "\n",
    "    predicted_class_id = sentiment_batch.argmax().item()\n",
    "    sentiment_label = bertweet_model.config.id2label[predicted_class_id]\n",
    "\n",
    "    fine_tuned_sentiment_results.append(\n",
    "        {\"review_text\": review_text, \"sentiment\": sentiment_label}\n",
    "    )\n",
    "\n",
    "    batch_time = time.time() - start_time\n",
    "    processing_times.append(batch_time)\n",
    "\n",
    "# Compute average and maximum values\n",
    "avg_time = sum(processing_times) / len(processing_times)\n",
    "max_time = max(processing_times)\n",
    "avg_tokens = sum(token_counts) / len(token_counts)\n",
    "max_tokens = max(token_counts)\n",
    "total_batches = len(processing_times)\n",
    "total_processing_time = sum(processing_times)\n",
    "total_tokens = sum(token_counts) \n",
    "throughput = total_tokens / total_processing_time\n",
    "\n",
    "print(\"Total number of batches:\", total_batches)\n",
    "print(\"Total time taken: {:.4f} seconds\".format(total_processing_time))\n",
    "print(\"Average time per batch: {:.4f} seconds\".format(avg_time))\n",
    "print(\"Maximum time for a batch: {:.4f} seconds\".format(max_time))\n",
    "print(\"Average batch size (in tokens): {:.2f}\".format(avg_tokens))\n",
    "print(\"Longest batch (in tokens):\", max_tokens)\n",
    "print(\"Throughput (in tokens/second):\", throughput)\n",
    "\n",
    "bertweet_sentiment_amazon_df = pd.DataFrame(fine_tuned_sentiment_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "bertweet_sentiment_amazon_df.to_csv(\n",
    "    \"bertweet_sentiment_analysis_amazon_results.csv\", index=False\n",
    ")\n",
    "\n",
    "print(\"Result saved to 'bertweet_sentiment_analysis_amazon_results.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4121061c-85e4-47ff-b6aa-097e4969a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from sentiment_analysis.config import logger\n",
    "from sentiment_analysis.utils import run_command\n",
    "from sentiment_analysis.load import load_amazon_reviews, load_model\n",
    "from sentiment_analysis.process import batch_sentiment_analysis\n",
    "import sentiment_analysis.process as process\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8d6a159-9dce-4548-8f2d-393a198b400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 8953020 bytes (8.54 MB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = \"Subscription_Boxes.jsonl\"\n",
    "size_bytes = os.path.getsize(file_path)\n",
    "size_mb = size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"File size: {size_bytes} bytes ({size_mb:.2f} MB)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdbcda99-2fde-4181-9da5-502e0a4557e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_cpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24b6ea23-4f27-4fdc-aab0-a8b3fc015300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 14:26:29,698 - INFO - Testing with target partition size of 8 MB...\n",
      "2025-04-08 14:26:29,701 - INFO - Using 4 partitions for 8 MB target size\n"
     ]
    }
   ],
   "source": [
    "target_mb = 8\n",
    "total_cores = 4\n",
    "task_cpus = int(spark.conf.get(\"spark.task.cpus\", \"1\"))\n",
    "min_partitions = total_cores // task_cpus\n",
    "logger.info(f\"Testing with target partition size of {target_mb} MB...\")\n",
    "    \n",
    "# Calculate target reviews per partition\n",
    "target_bytes = target_mb * 1024 * 1024\n",
    "\n",
    "# Calculate number of partitions\n",
    "num_partitions = max(min_partitions, math.ceil(size_bytes / target_bytes))\n",
    "\n",
    "logger.info(f\"Using {num_partitions} partitions for {target_mb} MB target size\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d2e6975-9b47-49d6-a83f-c73808761d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_results_csv_in_hdfs(read_hdfs_path, write_hdfs_path, csv_file_name):\n",
    "    logger.info(f\"WRITING ANALYSIS SUMMARY OUTPUT {csv_file_name} TO HDFS...\")\n",
    "    final_path = f\"{write_hdfs_path}/{csv_file_name}.csv\"\n",
    "    temp_csv_path = \"/tmp/merged_results.csv\"\n",
    "    # Merge csv files from hdfs and save them locally\n",
    "    merge_command = [\n",
    "        \"/home/almalinux/hadoop-3.4.0/bin/hdfs\",\n",
    "        \"dfs\",\n",
    "        \"-getmerge\",\n",
    "        f\"{read_hdfs_path}/part-*.csv\",\n",
    "        temp_csv_path,\n",
    "    ]\n",
    "    run_command(merge_command)\n",
    "\n",
    "    # Upload the merged csv to hdfs\n",
    "    upload_command = [\n",
    "        \"/home/almalinux/hadoop-3.4.0/bin/hdfs\",\n",
    "        \"dfs\",\n",
    "        \"-put\",\n",
    "        \"-f\",\n",
    "        temp_csv_path,\n",
    "        final_path,\n",
    "    ]\n",
    "    run_command(upload_command)\n",
    "    # Remove the local merged file\n",
    "    delete_local_file(temp_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41a6c39e-99c6-46f9-a7c8-a474c77f1cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/08 13:40:35 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "2025-04-08 13:40:42,666 - INFO - Processing 16,216 reviews                      \n",
      "2025-04-08 13:40:42,671 - INFO - Loading BERTweet model and tokenizer...\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji\n",
      "2025-04-08 13:40:58,267 - INFO - Processing 7674 reviews...         (0 + 3) / 3]\n",
      "2025-04-08 13:40:58,345 - INFO - Processing 1059 reviews...\n",
      "2025-04-08 13:40:58,815 - INFO - Processing reviews using 1 torch threads...\n",
      "2025-04-08 13:40:58,863 - INFO - Processing reviews using 1 torch threads...\n",
      "2025-04-08 13:40:59,092 - INFO - Processing 7483 reviews...\n",
      "2025-04-08 13:40:59,703 - INFO - Processing reviews using 1 torch threads...\n",
      "2025-04-08 13:44:14,791 - INFO - Done processing 1059 reviews..     (0 + 3) / 3]\n",
      "2025-04-08 13:50:02,343 - ERROR - KeyboardInterrupt while sending command.) / 3]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/almalinux/spark-3.5.5-bin-hadoop3-scala2.13/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/almalinux/spark-3.5.5-bin-hadoop3-scala2.13/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib64/python3.9/socket.py\", line 716, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "2025-04-08 13:50:02,357 - INFO - Closing down clientserver connection\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m\n\u001b[1;32m     19\u001b[0m sentiment_results_df \u001b[38;5;241m=\u001b[39m sentiment_results_df\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m     20\u001b[0m     col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masin\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     21\u001b[0m     col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult.score\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 27\u001b[0m \u001b[43msentiment_results_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     29\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone processing all reviews in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/spark-3.5.5-bin-hadoop3-scala2.13/python/pyspark/sql/readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[1;32m   1863\u001b[0m )\n\u001b[0;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark-3.5.5-bin-hadoop3-scala2.13/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/spark-3.5.5-bin-hadoop3-scala2.13/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/spark-3.5.5-bin-hadoop3-scala2.13/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/python3.9/socket.py:716\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 716\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_path, output_path = \"/Subscription_Boxes.jsonl\", \"/analysis_outputs\"\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SentimentAnalysis\").getOrCreate()\n",
    "# Load the dataset\n",
    "reviews_df = load_amazon_reviews(spark, input_path)\n",
    "# Count total reviews\n",
    "total_reviews = reviews_df.count()\n",
    "logger.info(f\"Processing {total_reviews:,} reviews\")\n",
    "# Load model and tokenizer\n",
    "tokenizer, model = load_model()\n",
    "# Broadcast model and tokenizer to all workers\n",
    "process.bc_tokenizer = spark.sparkContext.broadcast(tokenizer)\n",
    "process.bc_model = spark.sparkContext.broadcast(model)\n",
    "sentiment_results_df = reviews_df.withColumn(\n",
    "    \"result\",\n",
    "    process.batch_sentiment_analysis(reviews_df[\"text\"]),\n",
    ")\n",
    "# Flatten the result column\n",
    "sentiment_results_df = sentiment_results_df.select(\n",
    "    col(\"asin\"),\n",
    "    col(\"user_id\"),\n",
    "    col(\"result.review_text\"),\n",
    "    col(\"result.sentiment\"),\n",
    "    col(\"result.score\"),\n",
    ")\n",
    "start_time = time.time()\n",
    "sentiment_results_df.write.option(\"header\", \"true\").mode(\"overwrite\").csv(output_path)\n",
    "end_time = time.time()\n",
    "logger.info(f\"Done processing all reviews in {end_time - start_time:.2f} seconds\")\n",
    "# Combine results into a single csv file\n",
    "merge_results_csv_in_hdfs(\n",
    "    output_path, \"/summary_outputs\", \"sentiment_analysis_results\"\n",
    ")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61c0b2eb-b5b8-4dba-ada7-e810e8543e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cores (default parallelism): 36\n"
     ]
    }
   ],
   "source": [
    "total_cores = sc.defaultParallelism\n",
    "print(\"Total cores (default parallelism):\", total_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff3cba-dd05-48c2-bce2-7f3e01adef94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
