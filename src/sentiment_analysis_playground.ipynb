{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ba2e61-8002-484e-9b62-c433f2b4b14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a61a3464-5690-4cf7-8db8-e6c96796518e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3520d010-d22c-4777-8b08-e63fd1afb4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from sentiment_analysis.config import logger\n",
    "from sentiment_analysis.utils import run_command, delete_local_file\n",
    "from sentiment_analysis.load import load_amazon_reviews, load_model\n",
    "from sentiment_analysis.process import batch_sentiment_analysis\n",
    "import sentiment_analysis.process as process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a743052-d2a7-411b-ac8f-e8065222e585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_df_to_hdfs_csv(df, hdfs_path, csv_file_name):\n",
    "    logger.info(f\"WRITING ANALYSIS SUMMARY OUTPUT {csv_file_name} TO HDFS...\")\n",
    "    write_path = hdfs_path + csv_file_name\n",
    "    df.write.option(\"header\", \"true\").mode(\"overwrite\").csv(write_path)\n",
    "    hdfs_mv_cmd = [\n",
    "        \"/home/almalinux/hadoop-3.4.0/bin/hdfs\",\n",
    "        \"dfs\",\n",
    "        \"-mv\",\n",
    "        write_path + \"/part-00000-*.csv\",\n",
    "        write_path + \".csv\",\n",
    "    ]\n",
    "    run_command(hdfs_mv_cmd)\n",
    "    hdfs_rm_cmd = [\n",
    "        \"/home/almalinux/hadoop-3.4.0/bin/hdfs\",\n",
    "        \"dfs\",\n",
    "        \"-rm\",\n",
    "        \"-r\",\n",
    "        write_path,\n",
    "    ]\n",
    "    run_command(hdfs_rm_cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "070493f1-b423-4009-a212-a5da497455eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_results_csv_in_hdfs(read_hdfs_path, write_hdfs_path, csv_file_name):\n",
    "    logger.info(f\"WRITING ANALYSIS SUMMARY OUTPUT {csv_file_name} TO HDFS...\")\n",
    "    final_path = f\"{write_hdfs_path}/{csv_file_name}.csv\"\n",
    "    temp_csv_path = \"/tmp/merged_results.csv\"\n",
    "    # Merge csv files from hdfs and save them locally\n",
    "    merge_command = [\n",
    "        \"/home/almalinux/hadoop-3.4.0/bin/hdfs\",\n",
    "        \"dfs\",\n",
    "        \"-getmerge\",\n",
    "        f\"{read_hdfs_path}/part-*.csv\",\n",
    "        temp_csv_path,\n",
    "    ]\n",
    "    run_command(merge_command)\n",
    "\n",
    "    # Upload the merged csv to hdfs\n",
    "    upload_command = [\n",
    "        \"/home/almalinux/hadoop-3.4.0/bin/hdfs\",\n",
    "        \"dfs\",\n",
    "        \"-put\",\n",
    "        \"-f\",\n",
    "        temp_csv_path,\n",
    "        final_path,\n",
    "    ]\n",
    "    run_command(upload_command)\n",
    "    # Remove the local merged file\n",
    "    delete_local_file(temp_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a63cc95-f50a-49e7-b135-84db7f190163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 12:14:09,498 - INFO - WRITING ANALYSIS SUMMARY OUTPUT sentiment_analysis_results TO HDFS...\n",
      "2025-04-09 12:14:11,646 - INFO - Command /home/almalinux/hadoop-3.4.0/bin/hdfs dfs -getmerge /analysis_outputs/part-*.csv /tmp/merged_results.csv Output: \n",
      "2025-04-09 12:14:11,650 - ERROR - Command /home/almalinux/hadoop-3.4.0/bin/hdfs dfs -getmerge /analysis_outputs/part-*.csv /tmp/merged_results.csv Error: getmerge: `/analysis_outputs/part-*.csv': No such file or directory\n",
      "\n",
      "2025-04-09 12:14:13,906 - INFO - Command /home/almalinux/hadoop-3.4.0/bin/hdfs dfs -put -f /tmp/merged_results.csv /summary_outputs/sentiment_analysis_results.csv Output: \n",
      "2025-04-09 12:14:13,910 - INFO - /tmp/merged_results.csv local file has been deleted.\n"
     ]
    }
   ],
   "source": [
    "read_hdfs_path, write_hdfs_path = \"/analysis_outputs\", \"/summary_outputs\"\n",
    "merge_results_csv_in_hdfs(read_hdfs_path, write_hdfs_path, \"sentiment_analysis_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee95face-dc73-4967-9cc2-51bf8888a3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/09 12:16:44 WARN CacheManager: Asked to cache already cached data.        \n",
      "2025-04-09 12:16:44,953 - INFO - Processing 16,216 reviews\n",
      "2025-04-09 12:16:44,954 - INFO - Loading BERTweet model and tokenizer...\n",
      "25/04/09 12:16:49 WARN Utils: Suppressing exception in finally: Java heap space]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1(TorrentBroadcast.scala:360)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1$adapted(TorrentBroadcast.scala:360)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast$$$Lambda$2171/0x000000084101f040.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.finish(LZ4BlockOutputStream.java:257)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:195)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.close(ObjectOutputStream.java:1824)\n",
      "\tat java.base/java.io.ObjectOutputStream.close(ObjectOutputStream.java:741)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.close(JavaSerializer.scala:60)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$5(TorrentBroadcast.scala:367)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast$$$Lambda$2175/0x0000000841027040.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:73)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:367)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n",
      "\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n",
      "\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.broadcast(JavaSparkContext.scala:546)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "25/04/09 12:16:49 ERROR TorrentBroadcast: Store broadcast broadcast_14 fail, remove all pieces of the broadcast\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o22.broadcast.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1(TorrentBroadcast.scala:360)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1$adapted(TorrentBroadcast.scala:360)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$$Lambda$2171/0x000000084101f040.apply(Unknown Source)\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1849)\n\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:708)\n\tat org.apache.spark.util.Utils$.$anonfun$copyStream$1(Utils.scala:272)\n\tat org.apache.spark.util.Utils$$$Lambda$2275/0x000000084108d840.apply$mcJ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.util.Utils$.copyStream(Utils.scala:279)\n\tat org.apache.spark.api.python.PythonBroadcast.$anonfun$writeObject$1(PythonRDD.scala:772)\n\tat org.apache.spark.api.python.PythonBroadcast$$Lambda$3615/0x00000008415a0040.apply$mcJ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n\tat org.apache.spark.api.python.PythonBroadcast.writeObject(PythonRDD.scala:768)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat java.base/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)\n\tat java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)\n\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Broadcast model and tokenizer to all workers\u001b[39;00m\n\u001b[1;32m     12\u001b[0m process\u001b[38;5;241m.\u001b[39mbc_tokenizer \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mbroadcast(tokenizer)\n\u001b[0;32m---> 13\u001b[0m process\u001b[38;5;241m.\u001b[39mbc_model \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m sentiment_results_df \u001b[38;5;241m=\u001b[39m reviews_df\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     process\u001b[38;5;241m.\u001b[39mbatch_sentiment_analysis(reviews_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Flatten the result column\u001b[39;00m\n",
      "File \u001b[0;32m~/spark-3.5.5-bin-hadoop3-scala2.13/python/pyspark/context.py:1765\u001b[0m, in \u001b[0;36mSparkContext.broadcast\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbroadcast\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadcast[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1736\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1737\u001b[0m \u001b[38;5;124;03m    Broadcast a read-only variable to the cluster, returning a :class:`Broadcast`\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;124;03m    object for reading it in distributed functions. The variable will\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1763\u001b[0m \u001b[38;5;124;03m    >>> bc.destroy()\u001b[39;00m\n\u001b[1;32m   1764\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBroadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pickled_broadcast_vars\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark-3.5.5-bin-hadoop3-scala2.13/python/pyspark/broadcast.py:138\u001b[0m, in \u001b[0;36mBroadcast.__init__\u001b[0;34m(self, sc, value, pickle_registry, path, sock_file)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_encryption_enabled:\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_broadcast\u001b[38;5;241m.\u001b[39mwaitTillDataReceived()\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jbroadcast \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_broadcast\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pickle_registry \u001b[38;5;241m=\u001b[39m pickle_registry\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# we're on an executor\u001b[39;00m\n",
      "File \u001b[0;32m~/spark-3.5.5-bin-hadoop3-scala2.13/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/spark-3.5.5-bin-hadoop3-scala2.13/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/spark-3.5.5-bin-hadoop3-scala2.13/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.broadcast.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1(TorrentBroadcast.scala:360)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1$adapted(TorrentBroadcast.scala:360)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$$Lambda$2171/0x000000084101f040.apply(Unknown Source)\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1849)\n\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:708)\n\tat org.apache.spark.util.Utils$.$anonfun$copyStream$1(Utils.scala:272)\n\tat org.apache.spark.util.Utils$$$Lambda$2275/0x000000084108d840.apply$mcJ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.util.Utils$.copyStream(Utils.scala:279)\n\tat org.apache.spark.api.python.PythonBroadcast.$anonfun$writeObject$1(PythonRDD.scala:772)\n\tat org.apache.spark.api.python.PythonBroadcast$$Lambda$3615/0x00000008415a0040.apply$mcJ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n\tat org.apache.spark.api.python.PythonBroadcast.writeObject(PythonRDD.scala:768)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat java.base/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)\n\tat java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)\n\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 3) / 3]"
     ]
    }
   ],
   "source": [
    "input_path, output_path = \"/Subscription_Boxes.jsonl\", \"/analysis_outputs\"\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SentimentAnalysis\").getOrCreate()\n",
    "# Load the dataset\n",
    "reviews_df = load_amazon_reviews(spark, input_path)\n",
    "# Count total reviews\n",
    "total_reviews = reviews_df.count()\n",
    "logger.info(f\"Processing {total_reviews:,} reviews\")\n",
    "# Load model and tokenizer\n",
    "tokenizer, model = load_model()\n",
    "# Broadcast model and tokenizer to all workers\n",
    "process.bc_tokenizer = spark.sparkContext.broadcast(tokenizer)\n",
    "process.bc_model = spark.sparkContext.broadcast(model)\n",
    "sentiment_results_df = reviews_df.withColumn(\n",
    "    \"result\",\n",
    "    process.batch_sentiment_analysis(reviews_df[\"text\"]),\n",
    ")\n",
    "# Flatten the result column\n",
    "sentiment_results_df = sentiment_results_df.select(\n",
    "    col(\"asin\"),\n",
    "    col(\"user_id\"),\n",
    "    col(\"result.review_text\"),\n",
    "    col(\"result.sentiment\"),\n",
    "    col(\"result.score\"),\n",
    ")\n",
    "start_time = time.time()\n",
    "sentiment_results_df.write.option(\"header\", \"true\").mode(\"overwrite\").csv(output_path)\n",
    "end_time = time.time()\n",
    "logger.info(f\"Done processing all reviews in {end_time - start_time:.2f} seconds\")\n",
    "# Combine results into a single csv file\n",
    "merge_results_csv_in_hdfs(\n",
    "    output_path, \"/summary_outputs\", \"sentiment_analysis_results\"\n",
    ")\n",
    "# # Read the Parquet files\n",
    "# df = spark.read.parquet(output_path).coalesce(1)\n",
    "# # Write to CSV\n",
    "# write_df_to_hdfs_csv(\n",
    "#     df,\n",
    "#     \"/summary_outputs/\",\n",
    "#     \"sentiment_analysis_results\",\n",
    "# )\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9491921-6836-425e-af07-1541649faca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89eb259-793a-499b-9786-49dfadb103c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
